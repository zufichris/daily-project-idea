#  Self-Optimizing Traffic Light Simulation

## Overview
This project simulates a simplified traffic intersection with self-optimizing traffic light control using reinforcement learning.  The goal is to minimize average waiting time for vehicles at the intersection.  This is a smaller-scale, simplified version of a complex real-world problem, allowing for rapid prototyping and experimentation within a short timeframe.

## Technologies & Tools
- Python 3
- Pygame (for visualization)
- NumPy (for numerical computation)
- A reinforcement learning library (e.g., Stable Baselines3)

## Features & Requirements
- **Traffic Generation:**  Simulate vehicles arriving at the intersection randomly with varying arrival rates.
- **Traffic Light Control:** Implement a basic traffic light controller (initially, a simple timer-based system).
- **Reinforcement Learning Agent:** Train a reinforcement learning agent (e.g., using a Proximal Policy Optimization algorithm) to learn optimal traffic light switching patterns.
- **Performance Metrics:** Track average waiting time for vehicles.
- **Visualization:** Display the simulation using Pygame, showing vehicle movements and traffic light states.

- **Advanced Features (Optional):** Incorporate different vehicle types with varying speeds and priorities. Add a more realistic road network (e.g., multiple lanes).

## Implementation Steps
1. **Basic Simulation Setup:** Create a simple simulation environment using Pygame, showing the intersection and generating vehicles randomly. Implement a basic timer-based traffic light controller.
2. **Reinforcement Learning Environment:** Define the state space (e.g., number of vehicles waiting at each approach), action space (traffic light states), reward function (negative waiting time), and create an environment compatible with the chosen reinforcement learning library.
3. **Agent Training:** Train a reinforcement learning agent using the defined environment. Experiment with different hyperparameters to optimize performance.
4. **Performance Evaluation:** Evaluate the trained agent's performance by measuring the average waiting time compared to the baseline timer-based controller.
5. **Visualization Enhancement (Optional):** Improve the visualization by adding more detailed information (e.g., average waiting times displayed on the screen).

## Challenges & Considerations
- **Reward Function Design:**  Crafting an effective reward function that accurately reflects the desired behavior (minimizing waiting time) can be challenging.  Experimentation and iterative refinement are key.
- **Computational Cost:**  Training reinforcement learning models can be computationally intensive.  Simplifying the environment (e.g., reducing the number of lanes or vehicles) might be necessary to ensure completion within the time constraint.

## Learning Outcomes
- Reinforcement Learning Implementation: Gain practical experience in implementing and training a reinforcement learning agent for a real-world-inspired problem.
- Simulation Development:  Improve skills in creating and managing a discrete event simulation.

