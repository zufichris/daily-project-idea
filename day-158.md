# Mini-Game AI with Reinforcement Learning

## Overview

This project focuses on developing a simple AI agent for a classic game like Tic-Tac-Toe or Connect Four using reinforcement learning (RL). The goal is to build a functional, albeit rudimentary, RL agent capable of learning optimal strategies within a short timeframe.  This demonstrates a practical application of RL principles in a constrained environment.

## Technologies & Tools

- Programming Language: Python
- Libraries:  `gym` (for environment creation), `stable-baselines3` (for RL algorithms)  or a similar RL library.
- Tools:  A suitable Python IDE (VS Code, PyCharm).


## Features & Requirements

- **Core Features:**
    -  Creation of a game environment (Tic-Tac-Toe or Connect Four).
    -  Implementation of a simple RL agent (e.g., Q-learning or a simpler algorithm from `stable-baselines3`).
    -  Training the agent against a random opponent.
    -  Evaluation of the agent's performance against a random or simple rule-based opponent.
    -  Visualization of the training process (e.g., plotting win rates).

- **Advanced/Optional Features:**
    -  Implementation of a more sophisticated RL algorithm (e.g., Proximal Policy Optimization (PPO) from `stable-baselines3`).
    -  Saving and loading of trained agent models.


## Implementation Steps

1. **Environment Setup:** Define the game rules and create a game environment using `gym` or a similar framework.  This involves defining the state space, action space, reward function, and game logic.
2. **Agent Selection and Initialization:** Choose an appropriate RL algorithm (e.g., Q-learning or a simpler algorithm from `stable-baselines3`). Initialize the agent with default parameters.
3. **Training:** Train the agent by letting it play many games against a random opponent. Monitor the agent's performance using a metric like win rate.
4. **Evaluation:** Test the trained agent against a random or rule-based opponent to assess its learned strategy.
5. **Visualization (Optional):**  Plot win rates over training iterations to visualize the learning process.

## Challenges & Considerations

- **Computational resources:** Training RL agents can be computationally expensive, depending on the complexity of the algorithm and environment.  A simpler algorithm might be more suitable for a single-day project.
- **Hyperparameter tuning:** Finding optimal hyperparameters for the RL algorithm might require some experimentation. Focus on finding a balance between training time and performance.


## Learning Outcomes

- Reinforced understanding of reinforcement learning concepts (e.g., state, action, reward, policy).
- Practical experience in implementing and training a simple RL agent.

